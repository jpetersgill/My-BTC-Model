{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary modules.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# import the data.\n",
    "\n",
    "csv_file = 'train.csv'\n",
    "import_data = pd.read_csv(csv_file, encoding='latin-1')\n",
    "sentiment_data = import_data[['Sentiment', 'SentimentText']].copy()\n",
    "\n",
    "# split the data into training and testing data.\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        sentiment_data.SentimentText, \n",
    "        sentiment_data.Sentiment,\n",
    "        train_size=0.85, \n",
    "        random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# name the Porter Stemmer function\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english', max_features = 85\n",
    "    \n",
    ")\n",
    "\n",
    "# this might be data leakage !!!!!! so i changed it\n",
    "\n",
    "X_train_features = vectorizer.fit_transform(\n",
    "    X_train.tolist())\n",
    "\n",
    "X_test_features = vectorizer.fit_transform(\n",
    "    X_test.tolist())\n",
    "\n",
    "\n",
    "X_train_features_nd = X_train_features.toarray()\n",
    "X_test_features_nd = X_test_features.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print(count, tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.31      0.38      6408\n",
      "          1       0.60      0.76      0.67      8591\n",
      "\n",
      "avg / total       0.55      0.57      0.54     14999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#X_train, X_test, y_train, y_test  = train_test_split(\n",
    "#        corpus_data_features_nd[0:len(training_data)], \n",
    "#        training_data.Sentiment,\n",
    " #       train_size=0.85, \n",
    "  #      random_state=1234)\n",
    "\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train_features_nd, y=y_train)\n",
    "y_pred = log_model.predict(X_test_features_nd)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
