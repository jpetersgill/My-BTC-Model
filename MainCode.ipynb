{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary modules.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# name the sets of words I will be using for NLP.\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "english_words = set(words.words())\n",
    "\n",
    "# name the lemmatizing function.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a function, 'text_preprocessor', for which you input a tweet (or other piece of text)\n",
    "# and it produces a list of words and their respective frequencies of occurance in the text.\n",
    "\n",
    "def text_preprocessor(text):\n",
    "    \n",
    "    # tokenize the text into words and change all letters to lowercase.\n",
    "    \n",
    "    lowercase_text = text.lower()\n",
    "    tokenized_text = word_tokenize(lowercase_text)\n",
    "\n",
    "    # remove stopwords if members of the 'stopwords' corpus.\n",
    "    \n",
    "    filtered_text = []\n",
    "\n",
    "    for w in tokenized_text:\n",
    "        if w not in stop_words:\n",
    "            filtered_text.append(w)\n",
    "        \n",
    "    # lemmatize each word.\n",
    "    \n",
    "    lemmatized_text = []\n",
    "\n",
    "    for w in filtered_text:\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(w))\n",
    "    \n",
    "    # remove non-words if not members of the 'words' corpus.\n",
    "    \n",
    "    filtered_english_text = []\n",
    "\n",
    "    for w in lemmatized_text:\n",
    "        if w in english_words:\n",
    "            filtered_english_text.append(w)\n",
    "            \n",
    "    # count the frequency of each word.\n",
    "    \n",
    "    counted_text = Counter(filtered_english_text)\n",
    "    \n",
    "    return counted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Naive Bayes model has six parameters. The following function, 'model_trainer' has the training data \n",
    "# as its input and outputs these six values. Firstly, I define the function 'score_counter' which I will use\n",
    "# to count the number of positive and negative pieces of text in the training data.\n",
    "\n",
    "def score_counter(score, vector):\n",
    "    \n",
    "    return len([r for r in vector if r == score])\n",
    "\n",
    "\n",
    "# define the 'model_trainer' function. \n",
    "# This function uses both the predefined 'text_preprocessor' and 'score_counter' functions.\n",
    "\n",
    "\n",
    "def model_trainer(training_data):\n",
    "    \n",
    "    # seperate the rows into positive and negative sentiment.\n",
    "    \n",
    "    positive_rows = training_data.loc[training_data['Sentiment'] == 1]\n",
    "    negative_rows = training_data.loc[training_data['Sentiment'] == 0]\n",
    "    \n",
    "    # consider only the text columns and reindex both dataframes.\n",
    "    \n",
    "    negative_text = negative_rows['SentimentText']\n",
    "    indexed_negative_text = negative_text.reset_index(drop=True)\n",
    "    \n",
    "    positive_text = positive_rows['SentimentText']\n",
    "    indexed_positive_text = positive_text.reset_index(drop=True)\n",
    "    \n",
    "    # create a vector of all the sentiment polarities in the training data.\n",
    "    \n",
    "    data_polarities = training_data['Sentiment'].tolist()\n",
    "    \n",
    "    # using the 'score_counter' function count the number of positive and negative entries.\n",
    "\n",
    "    positive_count = score_counter(1, data_polarities)\n",
    "    negative_count = score_counter(0, data_polarities)\n",
    "\n",
    "    # compile all positive and negative text into single strings for counting.\n",
    "    \n",
    "    complete_positive_text = \"\"\n",
    "    complete_negative_text = \"\"\n",
    "\n",
    "    for i in range(0, positive_count):\n",
    "        complete_positive_text = complete_positive_text + \" \" + indexed_positive_text[i]\n",
    "\n",
    "    for i in range(0, negative_count):\n",
    "        complete_negative_text = complete_negative_text + \" \" + indexed_negative_text[i]\n",
    "    \n",
    "    # apply our text preprocessor to the aggregated pieces of text to give total word counts.\n",
    "    \n",
    "    positive_counts = text_preprocessor(complete_positive_text)\n",
    "    negative_counts = text_preprocessor(complete_negative_text)\n",
    "    \n",
    "    # calculate the Naive probabilities of positive and negative sentiments respectively.\n",
    "    \n",
    "    positive_prob = positive_count / len(data_polarities)\n",
    "    negative_prob = negative_count / len(data_polarities)\n",
    "    \n",
    "    # create a vector of parameters to output.\n",
    "    \n",
    "    parameters = [positive_counts, positive_prob, positive_count, negative_counts, negative_prob, negative_count]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a function that inputs the text and three of the parameters (which all relate to either the\n",
    "# positive or negative class). It ouputs a number proportional to the probability that the text lies in that class.\n",
    "# This function uses the predefined 'text_preprocessor' function.\n",
    "\n",
    "def class_prob_predictor(text, class_counts, class_prob, class_count):\n",
    "    \n",
    "    # set the initial prediciton to be 1.\n",
    "    \n",
    "    prediction = 1\n",
    "\n",
    "    # preprocess the input text.\n",
    "\n",
    "    text_counts = text_preprocessor(text)\n",
    "\n",
    "    # we now correct our prediction using the Naive Bayes algorithm.\n",
    "    \n",
    "    # For each word in the text, we take the number of times that word occured in our training data for the \n",
    "    # given class, add 1 to 'smooth' the value, and then divide by the total number of words in that class \n",
    "    # (plus the class_count to smooth the denominator).\n",
    "    \n",
    "    # Smoothing is necessary to ensure that we don't multiply the prediction by 0 if the word doesn't exist \n",
    "    # in our training data. Smoothing the denominator counts ensures that we keep things even.\n",
    "    \n",
    "    for word in text_counts:\n",
    "    \n",
    "        prediction *=  text_counts.get(word) * ((class_counts.get(word, 0) + 1) / (sum(class_counts.values()) + class_count))\n",
    "        \n",
    "  # multiply by the probability of the class existing in the training_data.\n",
    "    \n",
    "    relative_prob_of_class = prediction * class_prob\n",
    "    \n",
    "    return relative_prob_of_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function 'text_classifier' in which you input a piece of text and the parameters of the model\n",
    "# and it outputs the sentiment polarity of that piece of text, based on the model.\n",
    "# This function uses the predefined 'class_prob_predictor' function.\n",
    "\n",
    "def text_classifier(text, parameters):\n",
    "    \n",
    "    # calculate relative probabilies of the class of the text.\n",
    "    \n",
    "    relative_prob_positive = class_prob_predictor(text, parameters[0], parameters[1], parameters[2])\n",
    "    relative_prob_negative = class_prob_predictor(text, parameters[3], parameters[4], parameters[5])\n",
    "    \n",
    "    # calculate normalising constant as the sum of these 'relative' probabilities.\n",
    "    \n",
    "    normalising_constant = relative_prob_positive + relative_prob_negative\n",
    "    \n",
    "    # normalise the values to give actual probabilities.\n",
    "    if normalising_constant > 0:\n",
    "        \n",
    "        normalised_prob_positive = relative_prob_positive / normalising_constant\n",
    "        normalised_prob_negative = relative_prob_negative / normalising_constant\n",
    "    \n",
    "    # return the polarity, defined to be the probability that the text is positive.\n",
    "    \n",
    "        polarity = normalised_prob_positive\n",
    "    \n",
    "        return polarity\n",
    "    \n",
    "    else:\n",
    "        return relative_prob_positive\n",
    "    \n",
    "# this 'if' statement was added to avoid dividing by zero which gave an error when using some of the tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function 'binary_text_classifier' in which you input a piece of text and the parameters of the model\n",
    "# and it outputs a 0 for positive and a 1 for negative. This will be used to test the model's accuracy.\n",
    "# This function uses the predefined 'text_classifier' function.\n",
    "\n",
    "def binary_text_classifier(text, parameters):\n",
    "    \n",
    "    polarity = text_classifier(text, parameters)\n",
    "    \n",
    "    if polarity >= 0.5:\n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data.\n",
    "\n",
    "csv_file = 'train.csv'\n",
    "import_data = pd.read_csv(csv_file, encoding='latin-1')\n",
    "sentiment_data = import_data[['Sentiment', 'SentimentText']].copy()\n",
    "\n",
    "# split the data into training and testing data.\n",
    "\n",
    "training_data = sentiment_data[0:74999]\n",
    "testing_data = sentiment_data[75000:75499].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model with my model trainer function and the training data.\n",
    "# The 'my_model' function uses the predefined 'text_classifier' function.\n",
    "\n",
    "parameters = model_trainer(training_data)\n",
    "\n",
    "def my_model(text):\n",
    "    polarity = text_classifier(text, parameters)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8721350768536257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put an example sentence into the model to test it.\n",
    "\n",
    "my_model(\"#bitcoin is looking GREAT right now - Buy! Buy! Buy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which tests the quality of the model. It takes as input the parameters and testing_data\n",
    "# and returns the success rate of the model as a percentage.\n",
    "# This function uses the predefined 'binary_text_classifier'.\n",
    "\n",
    "def model_tester(parameters, test_data):\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # create a new column to tabulate the predictions.\n",
    "    \n",
    "    testing_data_copy = test_data.copy()\n",
    "    testing_data_copy['Prediction'] = test_data['Sentiment'].copy()\n",
    "    \n",
    "    # for each row, calculate the polarity of the text based on my model.\n",
    "\n",
    "    range_max = len(testing_data_copy.Prediction)\n",
    "\n",
    "    for i in range(0,range_max):\n",
    "        testing_data_copy['Prediction'][i] = binary_text_classifier(test_data['SentimentText'][i], parameters)\n",
    "    \n",
    "    # calculate the success rate of the model and print.\n",
    "    \n",
    "    success_rate = 100*(1 - mean_absolute_error(testing_data_copy['Sentiment'], testing_data_copy['Prediction']))\n",
    "    print(\"success rate:\", success_rate, \"%\")\n",
    "    \n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate: 74.74949899799599 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.74949899799599"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the quality of the model.\n",
    "\n",
    "model_tester(parameters, testing_data)\n",
    "\n",
    "# change this so it is cross validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our data file for the tweets.\n",
    "\n",
    "csv_file = 'bitcoin.csv'\n",
    "\n",
    "# read the csv file as a pandas dataframe.\n",
    "\n",
    "twitter_data = pd.read_csv(csv_file)\n",
    "\n",
    "# specify the columns to use.\n",
    "\n",
    "twitter_data2 = twitter_data[['timestamp', 'text', 'retweets', 'favorites']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the textblob module for Natural Language Processing (to obtain the subjectivity scores).\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# create 'polarity' and 'subjectivity' columns using textblob to analyse the text.\n",
    "# 'polarity' measures the positivity or negativity of the text, on a scale from -1 to 1.\n",
    "# 'subjectivity' measures how factual the text appears to be, on a scale from 0 to 1.\n",
    "\n",
    "twitter_data2['polarity']=twitter_data2['text'].apply(lambda tweet: my_model(tweet))\n",
    "twitter_data2['subjectivity']=twitter_data2['text'].apply(lambda tweet: TextBlob(tweet).sentiment.subjectivity)\n",
    "\n",
    "# this has now output both the polarity and subjectivity of each tweet into additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'text' column as we have retrieved the sentiment from it.\n",
    "\n",
    "twitter_data3 = twitter_data2.drop('text', axis=1)\n",
    "\n",
    "# replace the column names with clearer titles.\n",
    "\n",
    "twitter_data3.columns = ['Date', 'Retweets', 'Favourites', 'Polarity', 'Subjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data gives the dates and times in an unusable format.\n",
    "# convert the timestamps into DateTime format.\n",
    "\n",
    "twitter_data3['Date'] =  pd.to_datetime(twitter_data3['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any DateTimes which are null.\n",
    "\n",
    "twitter_data4 = twitter_data3[twitter_data3.Date.notnull()] \n",
    "\n",
    "# use the 'resample' function to replace the DateTimes with Dates.\n",
    "# this reduces the number of rows and replaces the values in each column with daily averages.\n",
    "\n",
    "twitter_data5 = twitter_data4.resample('d', on='Date').mean().dropna(how='all')\n",
    "\n",
    "# rename the columns as \"Average Polarity\" and \"Average Subjectivity\".\n",
    "\n",
    "twitter_data5.columns = ['Av. Polarity', 'Av. Subjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and read our data file for the daily BTC price.\n",
    "# this data includes Price, Open, Daily High, Daily Low, Volume and Percentage Change, but we will use\n",
    "# the % change as our y variable for predictions.\n",
    "\n",
    "csv_file2 = 'BTC_USD Bitfinex Historical Data-2.csv'\n",
    "btc_data = pd.read_csv(csv_file2)\n",
    "\n",
    "# rename variables, restricting to only necessary columns.\n",
    "\n",
    "btc_data2 = btc_data[['Date', 'Price', 'Vol.', 'Change %']].copy()\n",
    "\n",
    "# put the 'Date' column into DateTime format.\n",
    "\n",
    "btc_data2['Date'] =  pd.to_datetime(btc_data2['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our final DataFrame by merging the two DataFrames and matching based on Date.\n",
    "# this is now possible as both use the DateTime format.\n",
    "# We also drop rows with null values (as these arrise from errors in the way the data was recorded).\n",
    "\n",
    "merged_data = pd.merge(twitter_data5, btc_data2, how='outer', on='Date')\n",
    "merged_data2 = merged_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Av. Polarity  Av. Subjectivity   Price    Vol. Change %\n",
      "6  2018-03-13      0.530616          0.187391  9135.0   61.54     0.15\n",
      "7  2018-03-14      0.537747          0.191272  8186.6   78.81   -10.38\n",
      "8  2018-03-15      0.541601          0.222260  8252.9   82.58     0.81\n",
      "9  2018-03-16      0.515665          0.180636  8251.0   56.34    -0.02\n",
      "10 2018-03-17      0.502545          0.202575  7851.0   48.97    -4.85\n",
      "11 2018-03-18      0.549517          0.203636  8200.2   88.92     4.45\n",
      "12 2018-03-19      0.529867          0.217948  8600.1   73.28     4.88\n",
      "13 2018-03-20      0.519701          0.216737  8899.7   54.28     3.48\n",
      "14 2018-03-21      0.530689          0.199832  8900.1   42.86     0.00\n",
      "15 2018-03-22      0.534268          0.222412  8706.4   54.42    -2.18\n",
      "16 2018-03-23      0.511127          0.230296  8908.0   44.91     2.32\n",
      "17 2018-03-24      0.526595          0.229707  8535.0   44.21    -4.19\n",
      "18 2018-03-25      0.525538          0.216278  8445.1   30.69    -1.05\n",
      "19 2018-03-26      0.523705          0.199895  8119.1   56.38    -3.86\n",
      "20 2018-03-27      0.497064          0.199275  7784.5   50.41    -4.12\n",
      "21 2018-03-28      0.536844          0.237110  7936.1   35.08     1.95\n",
      "22 2018-03-29      0.518430          0.217588  7094.0   77.41   -10.61\n",
      "23 2018-03-30      0.545151          0.210159  6840.4  108.25    -3.57\n",
      "24 2018-03-31      0.510363          0.196285  6925.3   68.02     1.24\n",
      "25 2018-04-04      0.501732          0.213652  6785.9   53.72    -8.37\n",
      "26 2018-04-05      0.542505          0.234632  6765.0   45.85    -0.31\n",
      "27 2018-04-13      0.517274          0.202869  7885.8   59.91    -0.33\n",
      "28 2018-04-14      0.517744          0.170890  8002.9   26.52     1.48\n",
      "29 2018-04-15      0.511508          0.198333  8354.1   31.90     4.39\n",
      "30 2018-04-16      0.520459          0.185345  8056.2   37.56    -3.57\n",
      "31 2018-04-17      0.479737          0.200257  7889.9   28.23    -2.06\n",
      "32 2018-04-18      0.523491          0.195072  8166.3   29.54     3.50\n",
      "33 2018-04-19      0.542889          0.201561  8272.8   29.21     1.30\n",
      "34 2018-04-20      0.520559          0.205827  8866.1   50.90     7.17\n",
      "35 2018-04-21      0.529026          0.198350  8919.1   39.56     0.60\n",
      "36 2018-04-22      0.531958          0.218105  8800.0   31.43    -1.34\n",
      "37 2018-04-23      0.549591          0.189927  8940.0   23.26     1.59\n",
      "38 2018-04-24      0.539803          0.202501  9644.4   60.35     7.88\n",
      "39 2018-04-25      0.526861          0.201290  8878.4   74.49    -7.94\n",
      "40 2018-04-26      0.535970          0.214890  9279.3   43.21     4.52\n",
      "41 2018-04-27      0.531815          0.210996  8920.1   30.15    -3.87\n",
      "42 2018-04-28      0.551669          0.219093  9345.3   45.08     4.77\n",
      "43 2018-04-29      0.539373          0.185927  9398.6   27.31     0.57\n",
      "44 2018-04-30      0.514942          0.185562  9240.0   23.26    -1.69\n",
      "45 2018-05-04      0.533585          0.219280  9702.8   24.54    -0.61\n",
      "46 2018-05-05      0.519147          0.193373  9859.6   29.51     1.62\n",
      "47 2018-05-13      0.606547          0.197365  8683.6   22.90     2.61\n",
      "48 2018-05-14      0.549684          0.249710  8670.8   37.15    -0.15\n",
      "49 2018-05-15      0.558467          0.201226  8467.5   26.21    -2.34\n"
     ]
    }
   ],
   "source": [
    "# before we can use SciKit Learn's data science tools, the data needs to be readable numerically\n",
    "# by these models, meaning there cannot be commas, 'k's to represent thousands, or percentage signs.\n",
    "\n",
    "final_data = merged_data2.copy()\n",
    "final_data['Price'] = merged_data2['Price'].str.replace(',', '')\n",
    "final_data['Vol.'] = merged_data2['Vol.'].str.replace('K', '')\n",
    "final_data['Change %'] = merged_data2['Change %'].str.replace('%', '')\n",
    "\n",
    "# 'final_data' now contains our fully processed data ready for training the model.\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~jpetersgill/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a graph to show sentiment over time alongside BTC Price over time\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='jpetersgill', api_key='8DmzeEcfwFaMHtUGHaSB')\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "x_variable1 = final_data['Date']\n",
    "y_variable1 = final_data['Av. Polarity']\n",
    "\n",
    "# Create the first trace\n",
    "trace1 = go.Scatter(\n",
    "    x = x_variable1,\n",
    "    y = y_variable1,\n",
    "    name = 'Twitter Sentiment'\n",
    ")\n",
    "\n",
    "# create the second trace\n",
    "\n",
    "x_variable2 = final_data['Date']\n",
    "y_variable2 = final_data['Price']\n",
    "\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = x_variable2,\n",
    "    y = y_variable2,\n",
    "    name = 'BTC Price',\n",
    "    yaxis = 'y2'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='BTC Price against Daily Average Twitter Sentiment Polarity',\n",
    "    yaxis=dict(\n",
    "        title='Sentiment Polarity'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Value (USD)',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='multiple-axes-double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert some correlation analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = final_data['Change %']\n",
    "X1 = final_data.drop('Change %', axis=1)\n",
    "X2 = X1.drop('Date', axis=1)\n",
    "X3 = X2.drop('Price', axis=1)\n",
    "X = X3.drop('Vol.', axis=1)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.332727272727273\n"
     ]
    }
   ],
   "source": [
    "btc_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit model\n",
    "\n",
    "btc_model.fit(train_X, train_y)\n",
    "\n",
    "# get predicted prices on validation data and output the MAE\n",
    "\n",
    "val_predictions = btc_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  3\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  3\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  3\n",
      "2.2796363636363637\n"
     ]
    }
   ],
   "source": [
    "# define a function 'get_mae' which inputs the number of leaf nodes\n",
    "# and the training and test data, and outputs Mean Average Error in applying\n",
    "# our model\n",
    "\n",
    "def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(predictors_train, targ_train)\n",
    "    preds_val = model.predict(predictors_val)\n",
    "    mae = mean_absolute_error(targ_val, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\n",
    "\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "    \n",
    "# import the Random Forest Regressor package\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# apply the random forest model and print the MAE\n",
    "\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(train_X, train_y)\n",
    "btc_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, btc_preds))\n",
    "\n",
    "# we see that our best result is obtained using the random forest\n",
    "# regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the necessary keys for the Twitter API.\n",
    "\n",
    "API_key = \"1JqfUbxuD2NV9uUlID6Ohd1gg\"\n",
    "API_secret_key = \"MeMGtg42Sdh1IS4Ai2PM404qDtbic2vYKJWQARgSstprkQCTj3\"\n",
    "access_token = \"1225757852-QdwVGzucPhzGDJEwTiBlIIWK0BgTtcqKNYDXKuc\"\n",
    "access_token_secret = \"xhAgF1dBPs1cz7TY8VlpWwc4PEE0qrhsHQ8q0Z22dR2ML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy module and authenticate with access token.\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_key, API_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_data_frame(tweets):\n",
    "    \n",
    "    df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Text'])\n",
    "        \n",
    "    df['Timestamp'] = np.array([tweet.created_at for tweet in tweets])\n",
    "    df['No. Likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "    df['No. Retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "    df['Sentiment Polarity'] = np.array([my_model(tweet.text) for tweet in tweets])\n",
    "    df['Subjectivity'] = np.array([TextBlob(tweet.text).sentiment.subjectivity for tweet in tweets])\n",
    "    \n",
    "        \n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def tweet_fetcher(key_word, num_of_tweets):\n",
    "    fetched_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search, q=key_word+'-filter:retweets', lang='en', rpp=100).items(num_of_tweets):\n",
    "        \n",
    "        fetched_tweets.append(tweet)\n",
    "        pass\n",
    "    \n",
    "    df = tweets_to_data_frame(fetched_tweets)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tweet_fetcher('bitcoin', 100) #this number can be changed at any point!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Av. Polarity  Av. Subjectivity\n",
      "0      0.431897          0.328242\n"
     ]
    }
   ],
   "source": [
    "condensed_sample1 = sample.drop('Text', axis=1)\n",
    "condensed_sample2 = condensed_sample1.drop('Timestamp', axis=1)\n",
    "condensed_sample3 = condensed_sample2.drop('No. Likes', axis=1)\n",
    "condensed_sample = condensed_sample3.drop('No. Retweets', axis=1)\n",
    "mean_pol = condensed_sample['Sentiment Polarity'].mean(axis=0)\n",
    "mean_subj = condensed_sample['Subjectivity'].mean(axis=0)\n",
    "d = {'Av. Polarity': [mean_pol], 'Av. Subjectivity': [mean_subj]}\n",
    "input_data = pd.DataFrame(data=d)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.791]\n"
     ]
    }
   ],
   "source": [
    "my_btc_percentage_change_pred = forest_model.predict(input_data)\n",
    "print(my_btc_percentage_change_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
